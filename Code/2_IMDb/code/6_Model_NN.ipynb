{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2652e1fe-8032-4fa8-a48b-a44d3e3e348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For data preprocess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "myseed = 42069  # set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8420304f-218a-4509-896e-388ffe9ec901",
   "metadata": {},
   "source": [
    "！！設定要訓練哪個資料集！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b139b207-1c69-4ea2-9635-2f2f01cb4238",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_no = True\n",
    "dataset_onehot = False\n",
    "dataset_w2v = False\n",
    "dataset_bert = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852272c7-3560-49d3-aabe-e3891d2f1593",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_PATH = Path('/home/hsien/110/sta/final_pkfile/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12c09f5-125d-4023-8cc6-7c430969da3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hsien/110/sta/final_pkfile/imdb_df_genres_title_cgname.pk\n",
      "/home/hsien/110/sta/final_pkfile/w2v_crew_arr_50.pk\n",
      "/home/hsien/110/sta/final_pkfile/bestw2v_model_crew.wv.vectors.npy\n",
      "/home/hsien/110/sta/final_pkfile/cast_onehot_df.pk\n",
      "/home/hsien/110/sta/final_pkfile/w2v_cast_arr_50.pk\n",
      "/home/hsien/110/sta/final_pkfile/bestw2v_model_crew.syn1neg.npy\n",
      "/home/hsien/110/sta/final_pkfile/crew_onehot_df.pk\n",
      "/home/hsien/110/sta/final_pkfile/bestw2v_model_crew\n",
      "/home/hsien/110/sta/final_pkfile/bestw2v_model.wv.vectors.npy\n",
      "/home/hsien/110/sta/final_pkfile/bestw2v_model\n",
      "/home/hsien/110/sta/final_pkfile/bestw2v_model.syn1neg.npy\n",
      "/home/hsien/110/sta/final_pkfile/crew_50_sparse_df.pk\n",
      "/home/hsien/110/sta/final_pkfile/w2v_crew_arr.pk\n",
      "/home/hsien/110/sta/final_pkfile/cast_50_sparse_df.pk\n",
      "/home/hsien/110/sta/final_pkfile/w2v_cast_arr.pk\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk(load_PATH):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef641a9-e3a0-4f7d-9818-f6f048135cd5",
   "metadata": {},
   "source": [
    "# Load\n",
    "此檔已有\n",
    "1. genres和titleType的one-hot\n",
    "2. change_name欄位\n",
    "\n",
    "還沒刪除['originalTitle', 'endYear', 'titleType', 'genres', 'cast', 'crew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61413012-0e14-4806-af39-7cae79804491",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(load_PATH / 'imdb_df_genres_title_cgname.pk', 'rb') as f:\n",
    "    imdb_df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c56eaf-70a9-4375-9a09-0c50db3047ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = imdb_df.drop(columns=['originalTitle', 'endYear', 'titleType', 'genres', 'cast', 'crew', 'tconst', 'primaryTitle'], inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8fd1354-776e-4b80-a843-28e5ae935d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vec(wtv, cast_list): # 我用平均\n",
    "    if (np.array(sum(wtv.wv[c] for c in cast_list if wtv.wv.has_index_for(c))).shape) == (300,):\n",
    "        return np.array(sum(wtv.wv[c] for c in cast_list if wtv.wv.has_index_for(c))) / len(cast_list)\n",
    "    else:\n",
    "        return np.zeros((300,), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "242196a3-ce6f-4d28-a464-dbb73cb014e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_no:\n",
    "    X = np.array(df.drop(columns=['averageRating'], inplace=False).loc[:, :])\n",
    "    y = np.ravel(df.loc[:, df.columns == 'averageRating'])\n",
    "    \n",
    "elif dataset_onehot:\n",
    "    \n",
    "    with open(load_PATH / 'cast_onehot_df.pk', 'rb') as f:\n",
    "        cast_onehot_df = pickle.load(f)\n",
    "    with open(load_PATH / 'crew_onehot_df.pk', 'rb') as f:\n",
    "        crew_onehot_df = pickle.load(f)\n",
    "    # 補上cast one hot\n",
    "    df_one_hot = pd.concat((df, cast_onehot_df), axis=1)\n",
    "    df_one_hot = pd.concat((df_one_hot, crew_onehot_df), axis=1)\n",
    "    \n",
    "    X = df_one_hot.drop(columns=['averageRating'], inplace=False).loc[:, :]\n",
    "    y = np.ravel(df_one_hot.loc[:, df_one_hot.columns == 'averageRating'])\n",
    "#     one_hot_data_arr = np.concatenate((X, y.reshape(-1,1)), axis=1)\n",
    "\n",
    "elif dataset_w2v:\n",
    "    wtv_cast = Word2Vec.load('/home/jack/0_Other HW/SLDL Project/model/cast_50_model')\n",
    "    wtv_crew = Word2Vec.load('/home/jack/0_Other HW/SLDL Project/model/crew_50_model')\n",
    "\n",
    "    w2v_cast_arr = np.asarray(list(get_vec(wtv_cast, x) for x in imdb_df['cast']), dtype=\"float64\")\n",
    "    w2v_crew_arr = np.asarray(list(get_vec(wtv_crew, x) for x in imdb_df['crew']), dtype=\"float64\")\n",
    "    \n",
    "    X = np.array(df.drop(columns=['averageRating'], inplace=False).loc[:, :])\n",
    "    y = np.ravel(df.loc[:, df.columns == 'averageRating'])\n",
    "    \n",
    "    X = np.concatenate((X, w2v_cast_arr), axis=1)\n",
    "    X = np.concatenate((X, w2v_crew_arr), axis=1)\n",
    "#     w2v_data_arr = np.concatenate((X,y.reshape(-1,1)), axis=1)\n",
    "\n",
    "elif dataset_bert:\n",
    "    wtv_cast = Word2Vec.load('/home/jack/0_Other HW/SLDL Project/model/cast_50_model')\n",
    "    wtv_crew = Word2Vec.load('/home/jack/0_Other HW/SLDL Project/model/crew_50_model')\n",
    "\n",
    "    w2v_cast_arr = np.asarray(list(get_vec(wtv_cast, x) for x in imdb_df['cast']), dtype=\"float64\")\n",
    "    w2v_crew_arr = np.asarray(list(get_vec(wtv_crew, x) for x in imdb_df['crew']), dtype=\"float64\")\n",
    "    \n",
    "    with open('/home/jack/0_Other HW/SLDL Project/model/df_bert.pk', 'rb') as f:\n",
    "        df_bert = pickle.load(f)\n",
    "        \n",
    "    bert_arr = np.array(df_bert)\n",
    "    X = np.array(df.drop(columns=['averageRating'], inplace=False).loc[:, :])\n",
    "    y = np.ravel(df.loc[:, df.columns == 'averageRating'])\n",
    "    X = np.concatenate((X, w2v_cast_arr), axis=1)\n",
    "    X = np.concatenate((X, w2v_crew_arr), axis=1)\n",
    "    X = np.concatenate((X, bert_arr), axis=1)\n",
    "#     bert_data_arr = np.concatenate((X,y.reshape(-1,1)), axis=1)\n",
    "    \n",
    "else:\n",
    "    print('YOU NEED SET ONE TYPE AS TRUE!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53b484c6-72df-4387-9e36-5dfe8acb7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b54d93f-5bb5-4970-b781-3f4bdc52dcce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130252, 43)\n",
      "(32564, 43)\n",
      "(40705, 43)\n"
     ]
    }
   ],
   "source": [
    "tr_data = np.concatenate((X_train, y_train.reshape(-1,1)), axis=1)\n",
    "tt_data = np.concatenate((X_test, y_test.reshape(-1,1)), axis=1)\n",
    "dv_data = np.concatenate((X_valid, y_valid.reshape(-1,1)), axis=1)\n",
    "\n",
    "print(tr_data.shape)\n",
    "print(dv_data.shape)\n",
    "print(tt_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62b5462-0165-4251-9d20-dde6b085d836",
   "metadata": {},
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "991d40a9-d846-4a9a-93a0-d763812f642a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_path = 'covid.train.csv'  # path to training data\n",
    "# tt_path = 'covid.test.csv'   # path to testing data\n",
    "\n",
    "# !gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv\n",
    "# !gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv\n",
    "\n",
    "\"\"\"# **Import Some Packages**\"\"\"\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(myseed)\n",
    "torch.manual_seed(myseed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(myseed)\n",
    "\n",
    "\"\"\"# **Some Utilities**\n",
    "\n",
    "You do not need to modify this part.\n",
    "\"\"\"\n",
    "\n",
    "def get_device():\n",
    "    ''' Get device (if GPU is available, use GPU) '''\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def plot_learning_curve(loss_record, title=''):\n",
    "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
    "    total_steps = len(loss_record['train'])\n",
    "    x_1 = range(total_steps)\n",
    "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
    "    figure(figsize=(6, 4))\n",
    "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
    "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
    "    plt.ylim(0.0, 5.)\n",
    "    plt.xlabel('Training steps')\n",
    "    plt.ylabel('MSE loss')\n",
    "    plt.title('Learning curve of {}'.format(title))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
    "    ''' Plot prediction of your DNN '''\n",
    "    if preds is None or targets is None:\n",
    "        model.eval()\n",
    "        preds, targets = [], []\n",
    "        for x, y in dv_set:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                preds.append(pred.detach().cpu())\n",
    "                targets.append(y.detach().cpu())\n",
    "        preds = torch.cat(preds, dim=0).numpy()\n",
    "        targets = torch.cat(targets, dim=0).numpy()\n",
    "\n",
    "    figure(figsize=(5, 5))\n",
    "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
    "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
    "    plt.xlim(-0.2, lim)\n",
    "    plt.ylim(-0.2, lim)\n",
    "    plt.xlabel('ground truth value')\n",
    "    plt.ylabel('predicted value')\n",
    "    plt.title('Ground Truth v.s. Prediction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e19b95b-65a5-4baf-86b9-49da2e0f4a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class myDataset(Dataset):\n",
    "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
    "    def __init__(self,\n",
    "                 movies_arr,\n",
    "                 mode='train',\n",
    "                 target_only=False):\n",
    "        self.mode = mode\n",
    "\n",
    "        # Read data into numpy arrays\n",
    "#         with open(path, 'r') as fp:\n",
    "#             data = list(csv.reader(fp))\n",
    "#             data = np.array(data[1:])[:, 1:].astype(float)\n",
    "        data = movies_arr\n",
    "    \n",
    "#         if not target_only:\n",
    "        feats = list(range(movies_arr.shape[1] - 1))\n",
    "#         else:\n",
    "            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\n",
    "            # f_regression\n",
    "#             feats = list(range(40)) +[40, 42, 43, 57, 58, 60, 61, 75, 77, 78, 79]\n",
    "# [57,75] + [42,43,44] + [42+18,43+18,44+18] + [42+18+18,43+18+18,44+18+18] \n",
    "            \n",
    "#             pass\n",
    "\n",
    "        if mode == 'test':\n",
    "            # Testing data\n",
    "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
    "            data = data[:, feats]\n",
    "            self.data = torch.FloatTensor(data)\n",
    "        else:\n",
    "            # Training data (train/dev sets)\n",
    "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
    "            target = data[:, -1]\n",
    "            data = data[:, feats]\n",
    "            \n",
    "            # Splitting training data into train & dev sets\n",
    "            if mode == 'train':\n",
    "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
    "            elif mode == 'dev':\n",
    "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
    "            \n",
    "            # Convert data into PyTorch tensors\n",
    "            self.data = torch.FloatTensor(data[indices])\n",
    "            self.target = torch.FloatTensor(target[indices])\n",
    "\n",
    "        # Normalize features (you may remove this part to see what will happen)\n",
    "        # if mode != 'train':\n",
    "        #     with open(tr_path, 'r') as fp:\n",
    "        #         train_data = list(csv.reader(fp))\n",
    "        #         train_data = np.array(train_data[1:])[:, 1:].astype(float)\n",
    "        #         train_data = train_data[:, feats]\n",
    "        #         train_data = torch.FloatTensor(train_data[[i for i in range(len(data)) if i % 10 != 0]])\n",
    "        #     self.data[:, 40:] = \\\n",
    "        #         (self.data[:, 40:] - train_data[:, 40:].mean(dim=0, keepdim=True)) \\\n",
    "        #         / train_data[:, 40:].std(dim=0, keepdim=True)\n",
    "        # else:\n",
    "        #     self.data[:, 40:] = \\\n",
    "        #         (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) \\\n",
    "        #         / self.data[:, 40:].std(dim=0, keepdim=True)\n",
    "\n",
    "        self.dim = self.data.shape[1]\n",
    "\n",
    "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
    "              .format(mode, len(self.data), self.dim))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Returns one sample at a time\n",
    "        if self.mode in ['train', 'dev']:\n",
    "            # For training\n",
    "            return self.data[index], self.target[index]\n",
    "        else:\n",
    "            # For testing (no target)\n",
    "            return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the size of the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "\"\"\"## **DataLoader**\n",
    "\n",
    "A `DataLoader` loads data from a given `Dataset` into batches.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def prep_dataloader(data, mode, batch_size, n_jobs=0, target_only=False):\n",
    "    ''' Generates a dataset, then is put into a dataloader. '''\n",
    "    dataset = myDataset(data, mode=mode, target_only=target_only)  # Construct dataset\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size,\n",
    "        shuffle=(mode == 'train'), drop_last=False,\n",
    "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
    "    return dataloader\n",
    "\n",
    "\"\"\"# **Deep Neural Network**\n",
    "\n",
    "`NeuralNet` is an `nn.Module` designed for regression.\n",
    "The DNN consists of 2 fully-connected layers with ReLU activation.\n",
    "This module also included a function `cal_loss` for calculating loss.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNet(nn.Module):\n",
    "    ''' A simple fully-connected deep neural network '''\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNet, self).__init__()\n",
    "\n",
    "        # Define your neural network here\n",
    "        # TODO: How to modify this model to achieve better performance?\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "        # Mean squared error loss\n",
    "        self.criterion = nn.MSELoss(reduction='mean')\n",
    "\n",
    "    def forward(self, x):\n",
    "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
    "        return self.net(x).squeeze(1)\n",
    "\n",
    "    def cal_loss(self, pred, target):\n",
    "        ''' Calculate loss '''\n",
    "        # TODO: you may implement L2 regularization here\n",
    "        return torch.sqrt(self.criterion(pred, target)+10**(-9))\n",
    "\n",
    "\"\"\"# **Train/Dev/Test**\n",
    "\n",
    "## **Training**\n",
    "\"\"\"\n",
    "\n",
    "def train(tr_set, dv_set, model, config, device):\n",
    "    ''' DNN training '''\n",
    "\n",
    "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
    "        model.parameters(), **config['optim_hparas'])\n",
    "\n",
    "    min_mse = 1000.\n",
    "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
    "    early_stop_cnt = 0\n",
    "    epoch = 0\n",
    "    while epoch < n_epochs:\n",
    "        model.train()                           # set model to training mode\n",
    "        for x, y in tr_set:                     # iterate through the dataloader\n",
    "            optimizer.zero_grad()               # set gradient to zero\n",
    "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
    "            pred = model(x)                     # forward pass (compute output)\n",
    "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
    "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
    "            optimizer.step()                    # update model with optimizer\n",
    "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
    "\n",
    "        # After each epoch, test your model on the validation (development) set.\n",
    "        dev_mse = dev(dv_set, model, device)\n",
    "        if dev_mse < min_mse:\n",
    "            # Save model if your model improved\n",
    "            min_mse = dev_mse\n",
    "            print('Saving model (epoch = {:4d}, loss = {:.4f})'\n",
    "                .format(epoch + 1, min_mse))\n",
    "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
    "            early_stop_cnt = 0\n",
    "        else:\n",
    "            early_stop_cnt += 1\n",
    "\n",
    "        epoch += 1\n",
    "        loss_record['dev'].append(dev_mse)\n",
    "        if early_stop_cnt > config['early_stop']:\n",
    "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
    "            break\n",
    "\n",
    "    print('Finished training after {} epochs'.format(epoch))\n",
    "    return min_mse, loss_record\n",
    "\n",
    "\"\"\"## **Validation**\"\"\"\n",
    "\n",
    "def dev(dv_set, model, device):\n",
    "    model.eval()                                # set model to evalutation mode\n",
    "    total_loss = 0\n",
    "    for x, y in dv_set:                         # iterate through the dataloader\n",
    "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():                   # disable gradient calculation\n",
    "            pred = model(x)                     # forward pass (compute output)\n",
    "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
    "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
    "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\"\"\"## **Testing**\"\"\"\n",
    "\n",
    "def test(tt_set, model, device):\n",
    "    model.eval()                                # set model to evalutation mode\n",
    "    preds = []\n",
    "    for x in tt_set:                            # iterate through the dataloader\n",
    "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
    "        with torch.no_grad():                   # disable gradient calculation\n",
    "            pred = model(x)                     # forward pass (compute output)\n",
    "            preds.append(pred.detach().cpu())   # collect prediction\n",
    "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "622df4f2-ac5a-42c7-8fa8-f8080df42228",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
    "os.makedirs('none_models', exist_ok=True)  # The trained model will be saved to ./models/\n",
    "target_only = True                   # TODO: Using 40 states & 2 tested_positive features\n",
    "\n",
    "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
    "config = {\n",
    "    'n_epochs': 4000,                # maximum number of epochs\n",
    "    'batch_size': 32,               # mini-batch size for dataloader\n",
    "    'optimizer': 'Adam',              # optimization algorithm (optimizer in torch.optim)\n",
    "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
    "        'lr': .001 if dataset_bert else (0.0001 if dataset_w2v else 0.00001),                 # learning rate of SGD\n",
    "        #'momentum': 0.9,              # momentum for SGD\n",
    "        'weight_decay':1e-4,\n",
    "        'betas':(0.4, 0.999)\n",
    "    },\n",
    "    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)\n",
    "    'save_path': 'none_models/model.pth'  # your model will be saved here\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbcb37c5-d504-48fd-96d7-5af4f6a5c5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the train set of COVID19 Dataset (117226 samples found, each dim = 42)\n",
      "Finished reading the dev set of COVID19 Dataset (3257 samples found, each dim = 42)\n",
      "Finished reading the test set of COVID19 Dataset (40705 samples found, each dim = 42)\n"
     ]
    }
   ],
   "source": [
    "tr_set = prep_dataloader(tr_data, 'train', config['batch_size'], target_only=target_only)\n",
    "dv_set = prep_dataloader(dv_data, 'dev', config['batch_size'], target_only=target_only)\n",
    "tt_set = prep_dataloader(tt_data, 'test', config['batch_size'], target_only=target_only)\n",
    "\n",
    "model = NeuralNet(tr_set.dataset.dim).to(device)  # Construct model and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e546ed9-5664-4824-8910-e0b2933cb4b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model (epoch =    1, loss = 5.9935)\n",
      "Saving model (epoch =    2, loss = 3.8072)\n",
      "Saving model (epoch =    3, loss = 1.7289)\n",
      "Saving model (epoch =    4, loss = 1.5122)\n",
      "Saving model (epoch =    5, loss = 1.3940)\n",
      "Saving model (epoch =    6, loss = 1.3272)\n",
      "Saving model (epoch =    7, loss = 1.2809)\n",
      "Saving model (epoch =    8, loss = 1.2466)\n",
      "Saving model (epoch =    9, loss = 1.2199)\n",
      "Saving model (epoch =   10, loss = 1.1993)\n",
      "Saving model (epoch =   11, loss = 1.1832)\n",
      "Saving model (epoch =   12, loss = 1.1707)\n",
      "Saving model (epoch =   13, loss = 1.1610)\n",
      "Saving model (epoch =   14, loss = 1.1535)\n",
      "Saving model (epoch =   15, loss = 1.1476)\n",
      "Saving model (epoch =   16, loss = 1.1432)\n",
      "Saving model (epoch =   17, loss = 1.1398)\n",
      "Saving model (epoch =   18, loss = 1.1371)\n",
      "Saving model (epoch =   19, loss = 1.1349)\n",
      "Saving model (epoch =   20, loss = 1.1332)\n",
      "Saving model (epoch =   21, loss = 1.1314)\n",
      "Saving model (epoch =   22, loss = 1.1301)\n",
      "Saving model (epoch =   23, loss = 1.1290)\n",
      "Saving model (epoch =   24, loss = 1.1277)\n",
      "Saving model (epoch =   25, loss = 1.1267)\n",
      "Saving model (epoch =   26, loss = 1.1259)\n",
      "Saving model (epoch =   27, loss = 1.1248)\n",
      "Saving model (epoch =   28, loss = 1.1238)\n",
      "Saving model (epoch =   29, loss = 1.1232)\n",
      "Saving model (epoch =   30, loss = 1.1220)\n",
      "Saving model (epoch =   31, loss = 1.1212)\n",
      "Saving model (epoch =   32, loss = 1.1204)\n",
      "Saving model (epoch =   33, loss = 1.1197)\n",
      "Saving model (epoch =   34, loss = 1.1188)\n",
      "Saving model (epoch =   35, loss = 1.1182)\n",
      "Saving model (epoch =   36, loss = 1.1175)\n",
      "Saving model (epoch =   37, loss = 1.1170)\n",
      "Saving model (epoch =   38, loss = 1.1162)\n",
      "Saving model (epoch =   39, loss = 1.1156)\n",
      "Saving model (epoch =   40, loss = 1.1149)\n",
      "Saving model (epoch =   41, loss = 1.1144)\n",
      "Saving model (epoch =   42, loss = 1.1139)\n",
      "Saving model (epoch =   43, loss = 1.1134)\n",
      "Saving model (epoch =   44, loss = 1.1128)\n",
      "Saving model (epoch =   45, loss = 1.1121)\n",
      "Saving model (epoch =   46, loss = 1.1116)\n",
      "Saving model (epoch =   47, loss = 1.1112)\n",
      "Saving model (epoch =   48, loss = 1.1108)\n",
      "Saving model (epoch =   49, loss = 1.1102)\n",
      "Saving model (epoch =   50, loss = 1.1102)\n",
      "Saving model (epoch =   51, loss = 1.1093)\n",
      "Saving model (epoch =   52, loss = 1.1090)\n",
      "Saving model (epoch =   53, loss = 1.1085)\n",
      "Saving model (epoch =   54, loss = 1.1081)\n",
      "Saving model (epoch =   55, loss = 1.1074)\n",
      "Saving model (epoch =   56, loss = 1.1072)\n",
      "Saving model (epoch =   57, loss = 1.1067)\n",
      "Saving model (epoch =   58, loss = 1.1064)\n",
      "Saving model (epoch =   59, loss = 1.1060)\n",
      "Saving model (epoch =   60, loss = 1.1057)\n",
      "Saving model (epoch =   61, loss = 1.1053)\n",
      "Saving model (epoch =   62, loss = 1.1047)\n",
      "Saving model (epoch =   63, loss = 1.1044)\n",
      "Saving model (epoch =   64, loss = 1.1040)\n",
      "Saving model (epoch =   65, loss = 1.1039)\n",
      "Saving model (epoch =   66, loss = 1.1035)\n",
      "Saving model (epoch =   67, loss = 1.1031)\n",
      "Saving model (epoch =   68, loss = 1.1029)\n",
      "Saving model (epoch =   69, loss = 1.1027)\n",
      "Saving model (epoch =   70, loss = 1.1023)\n",
      "Saving model (epoch =   71, loss = 1.1021)\n",
      "Saving model (epoch =   72, loss = 1.1018)\n",
      "Saving model (epoch =   74, loss = 1.1014)\n",
      "Saving model (epoch =   75, loss = 1.1011)\n",
      "Saving model (epoch =   76, loss = 1.1010)\n",
      "Saving model (epoch =   77, loss = 1.1006)\n",
      "Saving model (epoch =   78, loss = 1.1006)\n",
      "Saving model (epoch =   79, loss = 1.1002)\n",
      "Saving model (epoch =   80, loss = 1.1002)\n",
      "Saving model (epoch =   81, loss = 1.0999)\n",
      "Saving model (epoch =   82, loss = 1.0997)\n",
      "Saving model (epoch =   83, loss = 1.0994)\n",
      "Saving model (epoch =   84, loss = 1.0993)\n",
      "Saving model (epoch =   85, loss = 1.0991)\n",
      "Saving model (epoch =   86, loss = 1.0988)\n",
      "Saving model (epoch =   87, loss = 1.0986)\n",
      "Saving model (epoch =   88, loss = 1.0985)\n",
      "Saving model (epoch =   89, loss = 1.0982)\n",
      "Saving model (epoch =   90, loss = 1.0982)\n",
      "Saving model (epoch =   91, loss = 1.0980)\n",
      "Saving model (epoch =   93, loss = 1.0977)\n",
      "Saving model (epoch =   94, loss = 1.0976)\n",
      "Saving model (epoch =   95, loss = 1.0975)\n",
      "Saving model (epoch =   96, loss = 1.0971)\n",
      "Saving model (epoch =   97, loss = 1.0971)\n",
      "Saving model (epoch =   98, loss = 1.0968)\n",
      "Saving model (epoch =   99, loss = 1.0967)\n",
      "Saving model (epoch =  100, loss = 1.0967)\n",
      "Saving model (epoch =  101, loss = 1.0965)\n",
      "Saving model (epoch =  102, loss = 1.0962)\n",
      "Saving model (epoch =  103, loss = 1.0960)\n",
      "Saving model (epoch =  105, loss = 1.0957)\n",
      "Saving model (epoch =  107, loss = 1.0955)\n",
      "Saving model (epoch =  108, loss = 1.0954)\n",
      "Saving model (epoch =  109, loss = 1.0953)\n",
      "Saving model (epoch =  110, loss = 1.0951)\n",
      "Saving model (epoch =  111, loss = 1.0950)\n",
      "Saving model (epoch =  112, loss = 1.0950)\n",
      "Saving model (epoch =  113, loss = 1.0947)\n",
      "Saving model (epoch =  114, loss = 1.0946)\n",
      "Saving model (epoch =  115, loss = 1.0945)\n",
      "Saving model (epoch =  116, loss = 1.0945)\n",
      "Saving model (epoch =  117, loss = 1.0943)\n",
      "Saving model (epoch =  118, loss = 1.0942)\n",
      "Saving model (epoch =  119, loss = 1.0940)\n",
      "Saving model (epoch =  120, loss = 1.0938)\n",
      "Saving model (epoch =  121, loss = 1.0938)\n",
      "Saving model (epoch =  122, loss = 1.0937)\n",
      "Saving model (epoch =  123, loss = 1.0936)\n",
      "Saving model (epoch =  124, loss = 1.0935)\n",
      "Saving model (epoch =  126, loss = 1.0934)\n",
      "Saving model (epoch =  127, loss = 1.0932)\n",
      "Saving model (epoch =  128, loss = 1.0931)\n",
      "Saving model (epoch =  129, loss = 1.0930)\n",
      "Saving model (epoch =  130, loss = 1.0929)\n",
      "Saving model (epoch =  132, loss = 1.0926)\n",
      "Saving model (epoch =  133, loss = 1.0924)\n",
      "Saving model (epoch =  134, loss = 1.0924)\n",
      "Saving model (epoch =  135, loss = 1.0922)\n",
      "Saving model (epoch =  137, loss = 1.0920)\n",
      "Saving model (epoch =  138, loss = 1.0919)\n",
      "Saving model (epoch =  139, loss = 1.0918)\n",
      "Saving model (epoch =  140, loss = 1.0916)\n",
      "Saving model (epoch =  141, loss = 1.0914)\n",
      "Saving model (epoch =  142, loss = 1.0912)\n",
      "Saving model (epoch =  143, loss = 1.0912)\n",
      "Saving model (epoch =  144, loss = 1.0912)\n",
      "Saving model (epoch =  145, loss = 1.0910)\n",
      "Saving model (epoch =  146, loss = 1.0909)\n",
      "Saving model (epoch =  147, loss = 1.0908)\n",
      "Saving model (epoch =  148, loss = 1.0906)\n",
      "Saving model (epoch =  149, loss = 1.0904)\n",
      "Saving model (epoch =  151, loss = 1.0903)\n",
      "Saving model (epoch =  152, loss = 1.0901)\n",
      "Saving model (epoch =  153, loss = 1.0900)\n",
      "Saving model (epoch =  154, loss = 1.0899)\n",
      "Saving model (epoch =  156, loss = 1.0897)\n",
      "Saving model (epoch =  157, loss = 1.0897)\n",
      "Saving model (epoch =  158, loss = 1.0896)\n",
      "Saving model (epoch =  159, loss = 1.0894)\n",
      "Saving model (epoch =  160, loss = 1.0893)\n",
      "Saving model (epoch =  161, loss = 1.0891)\n",
      "Saving model (epoch =  163, loss = 1.0889)\n",
      "Saving model (epoch =  165, loss = 1.0888)\n",
      "Saving model (epoch =  167, loss = 1.0887)\n",
      "Saving model (epoch =  168, loss = 1.0885)\n",
      "Saving model (epoch =  169, loss = 1.0885)\n",
      "Saving model (epoch =  170, loss = 1.0883)\n",
      "Saving model (epoch =  171, loss = 1.0881)\n",
      "Saving model (epoch =  172, loss = 1.0881)\n",
      "Saving model (epoch =  174, loss = 1.0878)\n",
      "Saving model (epoch =  175, loss = 1.0877)\n",
      "Saving model (epoch =  176, loss = 1.0876)\n",
      "Saving model (epoch =  177, loss = 1.0875)\n",
      "Saving model (epoch =  179, loss = 1.0873)\n",
      "Saving model (epoch =  180, loss = 1.0871)\n",
      "Saving model (epoch =  182, loss = 1.0871)\n",
      "Saving model (epoch =  183, loss = 1.0871)\n",
      "Saving model (epoch =  184, loss = 1.0870)\n",
      "Saving model (epoch =  185, loss = 1.0868)\n",
      "Saving model (epoch =  186, loss = 1.0867)\n",
      "Saving model (epoch =  187, loss = 1.0867)\n",
      "Saving model (epoch =  188, loss = 1.0867)\n",
      "Saving model (epoch =  189, loss = 1.0866)\n",
      "Saving model (epoch =  191, loss = 1.0864)\n",
      "Saving model (epoch =  193, loss = 1.0861)\n",
      "Saving model (epoch =  194, loss = 1.0859)\n",
      "Saving model (epoch =  197, loss = 1.0859)\n",
      "Saving model (epoch =  199, loss = 1.0857)\n",
      "Saving model (epoch =  201, loss = 1.0856)\n",
      "Saving model (epoch =  203, loss = 1.0855)\n",
      "Saving model (epoch =  204, loss = 1.0854)\n",
      "Saving model (epoch =  205, loss = 1.0854)\n",
      "Saving model (epoch =  206, loss = 1.0853)\n",
      "Saving model (epoch =  207, loss = 1.0852)\n",
      "Saving model (epoch =  209, loss = 1.0851)\n",
      "Saving model (epoch =  210, loss = 1.0850)\n",
      "Saving model (epoch =  211, loss = 1.0850)\n",
      "Saving model (epoch =  212, loss = 1.0849)\n",
      "Saving model (epoch =  214, loss = 1.0846)\n",
      "Saving model (epoch =  216, loss = 1.0846)\n",
      "Saving model (epoch =  219, loss = 1.0845)\n",
      "Saving model (epoch =  220, loss = 1.0844)\n",
      "Saving model (epoch =  222, loss = 1.0843)\n",
      "Saving model (epoch =  223, loss = 1.0843)\n",
      "Saving model (epoch =  224, loss = 1.0842)\n",
      "Saving model (epoch =  225, loss = 1.0841)\n",
      "Saving model (epoch =  227, loss = 1.0841)\n",
      "Saving model (epoch =  228, loss = 1.0839)\n",
      "Saving model (epoch =  230, loss = 1.0838)\n",
      "Saving model (epoch =  232, loss = 1.0838)\n",
      "Saving model (epoch =  233, loss = 1.0837)\n",
      "Saving model (epoch =  236, loss = 1.0835)\n",
      "Saving model (epoch =  239, loss = 1.0834)\n",
      "Saving model (epoch =  240, loss = 1.0834)\n",
      "Saving model (epoch =  243, loss = 1.0832)\n",
      "Saving model (epoch =  244, loss = 1.0832)\n",
      "Saving model (epoch =  245, loss = 1.0830)\n",
      "Saving model (epoch =  248, loss = 1.0830)\n",
      "Saving model (epoch =  252, loss = 1.0828)\n",
      "Saving model (epoch =  253, loss = 1.0828)\n",
      "Saving model (epoch =  257, loss = 1.0827)\n",
      "Saving model (epoch =  258, loss = 1.0825)\n",
      "Saving model (epoch =  260, loss = 1.0825)\n",
      "Saving model (epoch =  261, loss = 1.0824)\n",
      "Saving model (epoch =  262, loss = 1.0823)\n",
      "Saving model (epoch =  263, loss = 1.0823)\n",
      "Saving model (epoch =  265, loss = 1.0822)\n",
      "Saving model (epoch =  266, loss = 1.0822)\n",
      "Saving model (epoch =  268, loss = 1.0821)\n",
      "Saving model (epoch =  270, loss = 1.0820)\n",
      "Saving model (epoch =  275, loss = 1.0818)\n",
      "Saving model (epoch =  276, loss = 1.0818)\n",
      "Saving model (epoch =  278, loss = 1.0818)\n",
      "Saving model (epoch =  280, loss = 1.0817)\n",
      "Saving model (epoch =  283, loss = 1.0816)\n",
      "Saving model (epoch =  284, loss = 1.0815)\n",
      "Saving model (epoch =  290, loss = 1.0815)\n",
      "Saving model (epoch =  291, loss = 1.0814)\n",
      "Saving model (epoch =  292, loss = 1.0813)\n",
      "Saving model (epoch =  296, loss = 1.0811)\n",
      "Saving model (epoch =  298, loss = 1.0811)\n",
      "Saving model (epoch =  299, loss = 1.0810)\n",
      "Saving model (epoch =  305, loss = 1.0810)\n",
      "Saving model (epoch =  308, loss = 1.0808)\n",
      "Saving model (epoch =  310, loss = 1.0808)\n",
      "Saving model (epoch =  313, loss = 1.0807)\n",
      "Saving model (epoch =  319, loss = 1.0806)\n",
      "Saving model (epoch =  320, loss = 1.0805)\n",
      "Saving model (epoch =  321, loss = 1.0805)\n",
      "Saving model (epoch =  322, loss = 1.0804)\n",
      "Saving model (epoch =  325, loss = 1.0804)\n",
      "Saving model (epoch =  329, loss = 1.0802)\n",
      "Saving model (epoch =  334, loss = 1.0802)\n",
      "Saving model (epoch =  335, loss = 1.0801)\n",
      "Saving model (epoch =  337, loss = 1.0800)\n",
      "Saving model (epoch =  339, loss = 1.0800)\n",
      "Saving model (epoch =  340, loss = 1.0799)\n",
      "Saving model (epoch =  341, loss = 1.0799)\n",
      "Saving model (epoch =  343, loss = 1.0798)\n",
      "Saving model (epoch =  344, loss = 1.0798)\n",
      "Saving model (epoch =  346, loss = 1.0797)\n",
      "Saving model (epoch =  350, loss = 1.0796)\n",
      "Saving model (epoch =  352, loss = 1.0796)\n",
      "Saving model (epoch =  355, loss = 1.0795)\n",
      "Saving model (epoch =  357, loss = 1.0794)\n",
      "Saving model (epoch =  363, loss = 1.0793)\n",
      "Saving model (epoch =  364, loss = 1.0793)\n",
      "Saving model (epoch =  366, loss = 1.0792)\n",
      "Saving model (epoch =  369, loss = 1.0791)\n",
      "Saving model (epoch =  372, loss = 1.0790)\n",
      "Saving model (epoch =  376, loss = 1.0790)\n",
      "Saving model (epoch =  379, loss = 1.0788)\n",
      "Saving model (epoch =  388, loss = 1.0788)\n",
      "Saving model (epoch =  389, loss = 1.0787)\n",
      "Saving model (epoch =  396, loss = 1.0787)\n",
      "Saving model (epoch =  397, loss = 1.0786)\n",
      "Saving model (epoch =  398, loss = 1.0785)\n",
      "Saving model (epoch =  399, loss = 1.0785)\n",
      "Saving model (epoch =  401, loss = 1.0784)\n",
      "Saving model (epoch =  404, loss = 1.0784)\n",
      "Saving model (epoch =  406, loss = 1.0784)\n",
      "Saving model (epoch =  407, loss = 1.0783)\n",
      "Saving model (epoch =  408, loss = 1.0783)\n",
      "Saving model (epoch =  415, loss = 1.0782)\n",
      "Saving model (epoch =  416, loss = 1.0782)\n",
      "Saving model (epoch =  420, loss = 1.0782)\n",
      "Saving model (epoch =  421, loss = 1.0782)\n",
      "Saving model (epoch =  422, loss = 1.0780)\n",
      "Saving model (epoch =  423, loss = 1.0780)\n",
      "Saving model (epoch =  426, loss = 1.0780)\n",
      "Saving model (epoch =  428, loss = 1.0779)\n",
      "Saving model (epoch =  430, loss = 1.0779)\n",
      "Saving model (epoch =  433, loss = 1.0778)\n",
      "Saving model (epoch =  440, loss = 1.0777)\n",
      "Saving model (epoch =  442, loss = 1.0776)\n",
      "Saving model (epoch =  444, loss = 1.0776)\n",
      "Saving model (epoch =  448, loss = 1.0776)\n",
      "Saving model (epoch =  451, loss = 1.0775)\n",
      "Saving model (epoch =  454, loss = 1.0774)\n",
      "Saving model (epoch =  455, loss = 1.0774)\n",
      "Saving model (epoch =  456, loss = 1.0773)\n",
      "Saving model (epoch =  464, loss = 1.0773)\n",
      "Saving model (epoch =  468, loss = 1.0772)\n",
      "Saving model (epoch =  469, loss = 1.0772)\n",
      "Saving model (epoch =  471, loss = 1.0771)\n",
      "Saving model (epoch =  476, loss = 1.0770)\n",
      "Saving model (epoch =  479, loss = 1.0769)\n",
      "Saving model (epoch =  481, loss = 1.0769)\n",
      "Saving model (epoch =  486, loss = 1.0768)\n",
      "Saving model (epoch =  496, loss = 1.0768)\n",
      "Saving model (epoch =  500, loss = 1.0767)\n",
      "Saving model (epoch =  501, loss = 1.0766)\n",
      "Saving model (epoch =  507, loss = 1.0765)\n",
      "Saving model (epoch =  510, loss = 1.0765)\n",
      "Saving model (epoch =  512, loss = 1.0764)\n",
      "Saving model (epoch =  513, loss = 1.0764)\n",
      "Saving model (epoch =  515, loss = 1.0763)\n",
      "Saving model (epoch =  516, loss = 1.0763)\n",
      "Saving model (epoch =  520, loss = 1.0762)\n",
      "Saving model (epoch =  523, loss = 1.0762)\n",
      "Saving model (epoch =  526, loss = 1.0762)\n",
      "Saving model (epoch =  529, loss = 1.0761)\n",
      "Saving model (epoch =  531, loss = 1.0761)\n",
      "Saving model (epoch =  536, loss = 1.0761)\n",
      "Saving model (epoch =  538, loss = 1.0760)\n",
      "Saving model (epoch =  539, loss = 1.0760)\n",
      "Saving model (epoch =  541, loss = 1.0759)\n",
      "Saving model (epoch =  543, loss = 1.0759)\n",
      "Saving model (epoch =  547, loss = 1.0759)\n",
      "Saving model (epoch =  549, loss = 1.0759)\n",
      "Saving model (epoch =  550, loss = 1.0758)\n",
      "Saving model (epoch =  551, loss = 1.0757)\n",
      "Saving model (epoch =  557, loss = 1.0757)\n",
      "Saving model (epoch =  561, loss = 1.0756)\n",
      "Saving model (epoch =  564, loss = 1.0756)\n",
      "Saving model (epoch =  570, loss = 1.0756)\n",
      "Saving model (epoch =  571, loss = 1.0756)\n",
      "Saving model (epoch =  573, loss = 1.0755)\n",
      "Saving model (epoch =  574, loss = 1.0754)\n",
      "Saving model (epoch =  576, loss = 1.0754)\n",
      "Saving model (epoch =  579, loss = 1.0753)\n",
      "Saving model (epoch =  587, loss = 1.0753)\n",
      "Saving model (epoch =  588, loss = 1.0752)\n",
      "Saving model (epoch =  589, loss = 1.0752)\n",
      "Saving model (epoch =  594, loss = 1.0751)\n",
      "Saving model (epoch =  603, loss = 1.0751)\n",
      "Saving model (epoch =  604, loss = 1.0750)\n",
      "Saving model (epoch =  612, loss = 1.0750)\n",
      "Saving model (epoch =  613, loss = 1.0748)\n",
      "Saving model (epoch =  619, loss = 1.0748)\n",
      "Saving model (epoch =  624, loss = 1.0748)\n",
      "Saving model (epoch =  625, loss = 1.0747)\n",
      "Saving model (epoch =  632, loss = 1.0747)\n",
      "Saving model (epoch =  639, loss = 1.0746)\n",
      "Saving model (epoch =  647, loss = 1.0746)\n",
      "Saving model (epoch =  649, loss = 1.0745)\n",
      "Saving model (epoch =  652, loss = 1.0745)\n",
      "Saving model (epoch =  654, loss = 1.0744)\n",
      "Saving model (epoch =  661, loss = 1.0744)\n",
      "Saving model (epoch =  662, loss = 1.0744)\n",
      "Saving model (epoch =  663, loss = 1.0743)\n",
      "Saving model (epoch =  667, loss = 1.0743)\n",
      "Saving model (epoch =  669, loss = 1.0742)\n",
      "Saving model (epoch =  673, loss = 1.0742)\n",
      "Saving model (epoch =  682, loss = 1.0741)\n",
      "Saving model (epoch =  690, loss = 1.0740)\n",
      "Saving model (epoch =  692, loss = 1.0739)\n",
      "Saving model (epoch =  700, loss = 1.0739)\n",
      "Saving model (epoch =  703, loss = 1.0739)\n",
      "Saving model (epoch =  713, loss = 1.0737)\n",
      "Saving model (epoch =  723, loss = 1.0737)\n",
      "Saving model (epoch =  727, loss = 1.0737)\n",
      "Saving model (epoch =  734, loss = 1.0736)\n",
      "Saving model (epoch =  736, loss = 1.0736)\n",
      "Saving model (epoch =  742, loss = 1.0735)\n",
      "Saving model (epoch =  755, loss = 1.0734)\n",
      "Saving model (epoch =  757, loss = 1.0734)\n",
      "Saving model (epoch =  766, loss = 1.0733)\n",
      "Saving model (epoch =  768, loss = 1.0732)\n",
      "Saving model (epoch =  772, loss = 1.0732)\n",
      "Saving model (epoch =  785, loss = 1.0732)\n",
      "Saving model (epoch =  789, loss = 1.0730)\n",
      "Saving model (epoch =  796, loss = 1.0730)\n",
      "Saving model (epoch =  801, loss = 1.0730)\n",
      "Saving model (epoch =  803, loss = 1.0729)\n",
      "Saving model (epoch =  834, loss = 1.0725)\n",
      "Saving model (epoch =  842, loss = 1.0725)\n",
      "Saving model (epoch =  849, loss = 1.0725)\n",
      "Saving model (epoch =  852, loss = 1.0725)\n",
      "Saving model (epoch =  855, loss = 1.0724)\n",
      "Saving model (epoch =  858, loss = 1.0724)\n",
      "Saving model (epoch =  872, loss = 1.0722)\n",
      "Saving model (epoch =  879, loss = 1.0721)\n",
      "Saving model (epoch =  880, loss = 1.0721)\n",
      "Saving model (epoch =  886, loss = 1.0720)\n",
      "Saving model (epoch =  889, loss = 1.0720)\n",
      "Saving model (epoch =  892, loss = 1.0720)\n",
      "Saving model (epoch =  901, loss = 1.0719)\n",
      "Saving model (epoch =  905, loss = 1.0717)\n",
      "Saving model (epoch =  909, loss = 1.0717)\n",
      "Saving model (epoch =  916, loss = 1.0717)\n",
      "Saving model (epoch =  921, loss = 1.0716)\n",
      "Saving model (epoch =  925, loss = 1.0716)\n",
      "Saving model (epoch =  929, loss = 1.0715)\n",
      "Saving model (epoch =  931, loss = 1.0714)\n",
      "Saving model (epoch =  935, loss = 1.0714)\n",
      "Saving model (epoch =  940, loss = 1.0714)\n",
      "Saving model (epoch =  941, loss = 1.0714)\n",
      "Saving model (epoch =  944, loss = 1.0711)\n",
      "Saving model (epoch =  959, loss = 1.0710)\n",
      "Saving model (epoch =  962, loss = 1.0710)\n",
      "Saving model (epoch =  977, loss = 1.0707)\n",
      "Saving model (epoch =  983, loss = 1.0706)\n",
      "Saving model (epoch =  990, loss = 1.0706)\n",
      "Saving model (epoch =  992, loss = 1.0705)\n",
      "Saving model (epoch =  996, loss = 1.0704)\n",
      "Saving model (epoch =  997, loss = 1.0703)\n",
      "Saving model (epoch = 1009, loss = 1.0702)\n",
      "Saving model (epoch = 1019, loss = 1.0700)\n",
      "Saving model (epoch = 1025, loss = 1.0700)\n",
      "Saving model (epoch = 1038, loss = 1.0700)\n",
      "Saving model (epoch = 1039, loss = 1.0700)\n",
      "Saving model (epoch = 1042, loss = 1.0699)\n",
      "Saving model (epoch = 1045, loss = 1.0698)\n",
      "Saving model (epoch = 1052, loss = 1.0697)\n",
      "Saving model (epoch = 1056, loss = 1.0697)\n",
      "Saving model (epoch = 1067, loss = 1.0696)\n",
      "Saving model (epoch = 1076, loss = 1.0695)\n",
      "Saving model (epoch = 1080, loss = 1.0695)\n",
      "Saving model (epoch = 1081, loss = 1.0694)\n",
      "Saving model (epoch = 1087, loss = 1.0694)\n",
      "Saving model (epoch = 1094, loss = 1.0694)\n",
      "Saving model (epoch = 1108, loss = 1.0693)\n",
      "Saving model (epoch = 1114, loss = 1.0693)\n",
      "Saving model (epoch = 1120, loss = 1.0692)\n",
      "Saving model (epoch = 1122, loss = 1.0692)\n",
      "Saving model (epoch = 1136, loss = 1.0691)\n",
      "Saving model (epoch = 1138, loss = 1.0690)\n",
      "Saving model (epoch = 1145, loss = 1.0690)\n",
      "Saving model (epoch = 1161, loss = 1.0689)\n",
      "Saving model (epoch = 1168, loss = 1.0688)\n",
      "Saving model (epoch = 1179, loss = 1.0688)\n",
      "Saving model (epoch = 1186, loss = 1.0687)\n",
      "Saving model (epoch = 1201, loss = 1.0687)\n",
      "Saving model (epoch = 1205, loss = 1.0686)\n",
      "Saving model (epoch = 1211, loss = 1.0686)\n",
      "Saving model (epoch = 1212, loss = 1.0685)\n",
      "Saving model (epoch = 1221, loss = 1.0685)\n",
      "Saving model (epoch = 1222, loss = 1.0685)\n",
      "Saving model (epoch = 1224, loss = 1.0684)\n",
      "Saving model (epoch = 1236, loss = 1.0684)\n",
      "Saving model (epoch = 1237, loss = 1.0683)\n",
      "Saving model (epoch = 1238, loss = 1.0683)\n",
      "Saving model (epoch = 1245, loss = 1.0682)\n",
      "Saving model (epoch = 1247, loss = 1.0682)\n",
      "Saving model (epoch = 1248, loss = 1.0682)\n",
      "Saving model (epoch = 1260, loss = 1.0682)\n",
      "Saving model (epoch = 1269, loss = 1.0681)\n",
      "Saving model (epoch = 1281, loss = 1.0681)\n",
      "Saving model (epoch = 1286, loss = 1.0681)\n",
      "Saving model (epoch = 1290, loss = 1.0680)\n",
      "Saving model (epoch = 1291, loss = 1.0679)\n",
      "Saving model (epoch = 1297, loss = 1.0678)\n",
      "Saving model (epoch = 1343, loss = 1.0678)\n",
      "Saving model (epoch = 1358, loss = 1.0677)\n",
      "Saving model (epoch = 1477, loss = 1.0677)\n",
      "Saving model (epoch = 1483, loss = 1.0676)\n",
      "Saving model (epoch = 1499, loss = 1.0676)\n",
      "Saving model (epoch = 1505, loss = 1.0675)\n",
      "Saving model (epoch = 1512, loss = 1.0674)\n",
      "Saving model (epoch = 1518, loss = 1.0674)\n",
      "Saving model (epoch = 1522, loss = 1.0673)\n",
      "Saving model (epoch = 1535, loss = 1.0673)\n",
      "Saving model (epoch = 1544, loss = 1.0670)\n",
      "Saving model (epoch = 1548, loss = 1.0670)\n",
      "Saving model (epoch = 1551, loss = 1.0670)\n",
      "Saving model (epoch = 1556, loss = 1.0669)\n",
      "Saving model (epoch = 1567, loss = 1.0669)\n",
      "Saving model (epoch = 1568, loss = 1.0668)\n",
      "Saving model (epoch = 1576, loss = 1.0667)\n",
      "Saving model (epoch = 1584, loss = 1.0667)\n",
      "Saving model (epoch = 1585, loss = 1.0665)\n",
      "Saving model (epoch = 1610, loss = 1.0665)\n",
      "Saving model (epoch = 1618, loss = 1.0663)\n",
      "Saving model (epoch = 1621, loss = 1.0662)\n",
      "Saving model (epoch = 1630, loss = 1.0662)\n",
      "Saving model (epoch = 1631, loss = 1.0661)\n",
      "Saving model (epoch = 1643, loss = 1.0661)\n",
      "Saving model (epoch = 1650, loss = 1.0660)\n",
      "Saving model (epoch = 1652, loss = 1.0660)\n",
      "Saving model (epoch = 1657, loss = 1.0660)\n",
      "Saving model (epoch = 1659, loss = 1.0659)\n",
      "Saving model (epoch = 1663, loss = 1.0659)\n",
      "Saving model (epoch = 1666, loss = 1.0658)\n",
      "Saving model (epoch = 1692, loss = 1.0658)\n",
      "Saving model (epoch = 1693, loss = 1.0658)\n",
      "Saving model (epoch = 1697, loss = 1.0657)\n",
      "Saving model (epoch = 1702, loss = 1.0657)\n",
      "Saving model (epoch = 1708, loss = 1.0657)\n",
      "Saving model (epoch = 1712, loss = 1.0656)\n",
      "Saving model (epoch = 1731, loss = 1.0656)\n",
      "Saving model (epoch = 1736, loss = 1.0655)\n",
      "Saving model (epoch = 1740, loss = 1.0654)\n",
      "Saving model (epoch = 1766, loss = 1.0654)\n",
      "Saving model (epoch = 1767, loss = 1.0654)\n",
      "Saving model (epoch = 1769, loss = 1.0654)\n",
      "Saving model (epoch = 1779, loss = 1.0652)\n",
      "Saving model (epoch = 1797, loss = 1.0651)\n",
      "Saving model (epoch = 1803, loss = 1.0651)\n",
      "Saving model (epoch = 1852, loss = 1.0650)\n",
      "Saving model (epoch = 1856, loss = 1.0648)\n",
      "Saving model (epoch = 1981, loss = 1.0648)\n",
      "Saving model (epoch = 2008, loss = 1.0646)\n",
      "Saving model (epoch = 2014, loss = 1.0645)\n",
      "Saving model (epoch = 2062, loss = 1.0645)\n",
      "Saving model (epoch = 2073, loss = 1.0645)\n",
      "Saving model (epoch = 2088, loss = 1.0644)\n",
      "Saving model (epoch = 2110, loss = 1.0644)\n",
      "Saving model (epoch = 2126, loss = 1.0644)\n",
      "Saving model (epoch = 2134, loss = 1.0642)\n",
      "Saving model (epoch = 2166, loss = 1.0640)\n",
      "Saving model (epoch = 2174, loss = 1.0640)\n",
      "Saving model (epoch = 2207, loss = 1.0638)\n",
      "Saving model (epoch = 2259, loss = 1.0637)\n",
      "Saving model (epoch = 2313, loss = 1.0636)\n",
      "Saving model (epoch = 2325, loss = 1.0635)\n",
      "Saving model (epoch = 2369, loss = 1.0634)\n",
      "Saving model (epoch = 2377, loss = 1.0633)\n",
      "Saving model (epoch = 2387, loss = 1.0633)\n",
      "Saving model (epoch = 2394, loss = 1.0633)\n",
      "Saving model (epoch = 2407, loss = 1.0631)\n",
      "Saving model (epoch = 2412, loss = 1.0631)\n",
      "Saving model (epoch = 2424, loss = 1.0629)\n",
      "Saving model (epoch = 2443, loss = 1.0629)\n",
      "Saving model (epoch = 2459, loss = 1.0627)\n",
      "Saving model (epoch = 2525, loss = 1.0626)\n",
      "Saving model (epoch = 2558, loss = 1.0625)\n",
      "Saving model (epoch = 2571, loss = 1.0624)\n",
      "Saving model (epoch = 2573, loss = 1.0624)\n",
      "Saving model (epoch = 2579, loss = 1.0623)\n",
      "Saving model (epoch = 2592, loss = 1.0622)\n",
      "Saving model (epoch = 2602, loss = 1.0621)\n",
      "Saving model (epoch = 2622, loss = 1.0621)\n",
      "Saving model (epoch = 2629, loss = 1.0621)\n",
      "Saving model (epoch = 2639, loss = 1.0621)\n",
      "Saving model (epoch = 2654, loss = 1.0620)\n",
      "Saving model (epoch = 2655, loss = 1.0618)\n",
      "Saving model (epoch = 2657, loss = 1.0618)\n",
      "Saving model (epoch = 2658, loss = 1.0618)\n",
      "Saving model (epoch = 2689, loss = 1.0617)\n",
      "Saving model (epoch = 2724, loss = 1.0615)\n",
      "Saving model (epoch = 2777, loss = 1.0615)\n",
      "Saving model (epoch = 2781, loss = 1.0614)\n",
      "Saving model (epoch = 2809, loss = 1.0614)\n",
      "Saving model (epoch = 2810, loss = 1.0612)\n",
      "Saving model (epoch = 2813, loss = 1.0612)\n",
      "Saving model (epoch = 2841, loss = 1.0612)\n",
      "Saving model (epoch = 2847, loss = 1.0610)\n",
      "Saving model (epoch = 2888, loss = 1.0610)\n",
      "Saving model (epoch = 2905, loss = 1.0609)\n",
      "Saving model (epoch = 2908, loss = 1.0609)\n",
      "Saving model (epoch = 2961, loss = 1.0609)\n",
      "Saving model (epoch = 2966, loss = 1.0609)\n",
      "Saving model (epoch = 2977, loss = 1.0608)\n",
      "Saving model (epoch = 2979, loss = 1.0606)\n",
      "Saving model (epoch = 3016, loss = 1.0606)\n",
      "Saving model (epoch = 3050, loss = 1.0606)\n",
      "Saving model (epoch = 3055, loss = 1.0606)\n",
      "Saving model (epoch = 3056, loss = 1.0604)\n",
      "Saving model (epoch = 3073, loss = 1.0603)\n",
      "Saving model (epoch = 3160, loss = 1.0601)\n",
      "Saving model (epoch = 3199, loss = 1.0601)\n",
      "Saving model (epoch = 3201, loss = 1.0601)\n",
      "Saving model (epoch = 3202, loss = 1.0600)\n",
      "Saving model (epoch = 3223, loss = 1.0598)\n",
      "Saving model (epoch = 3310, loss = 1.0598)\n",
      "Saving model (epoch = 3320, loss = 1.0597)\n",
      "Saving model (epoch = 3326, loss = 1.0596)\n",
      "Saving model (epoch = 3331, loss = 1.0596)\n",
      "Saving model (epoch = 3339, loss = 1.0595)\n",
      "Saving model (epoch = 3353, loss = 1.0595)\n",
      "Saving model (epoch = 3354, loss = 1.0593)\n",
      "Saving model (epoch = 3433, loss = 1.0592)\n",
      "Saving model (epoch = 3437, loss = 1.0591)\n",
      "Saving model (epoch = 3465, loss = 1.0590)\n",
      "Saving model (epoch = 3492, loss = 1.0590)\n",
      "Saving model (epoch = 3506, loss = 1.0590)\n",
      "Saving model (epoch = 3558, loss = 1.0587)\n",
      "Saving model (epoch = 3596, loss = 1.0587)\n",
      "Saving model (epoch = 3610, loss = 1.0585)\n",
      "Saving model (epoch = 3614, loss = 1.0585)\n",
      "Saving model (epoch = 3631, loss = 1.0583)\n",
      "Saving model (epoch = 3660, loss = 1.0582)\n",
      "Saving model (epoch = 3687, loss = 1.0582)\n",
      "Saving model (epoch = 3725, loss = 1.0582)\n",
      "Saving model (epoch = 3739, loss = 1.0581)\n",
      "Saving model (epoch = 3741, loss = 1.0580)\n",
      "Saving model (epoch = 3749, loss = 1.0580)\n",
      "Saving model (epoch = 3777, loss = 1.0580)\n",
      "Saving model (epoch = 3810, loss = 1.0578)\n",
      "Saving model (epoch = 3837, loss = 1.0575)\n",
      "Finished training after 4000 epochs\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start Training\n",
    "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)\n",
    "plot_learning_curve(model_loss_record, title='deep model')\n",
    "\n",
    "del model\n",
    "model = NeuralNet(tr_set.dataset.dim).to(device)\n",
    "ckpt = torch.load(config['save_path'], map_location='cpu')  # Load your best model\n",
    "model.load_state_dict(ckpt)\n",
    "plot_pred(dv_set, model, device)  # Show prediction on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "465fbcab-c09b-4434-964e-9ce305b0855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a9127f-63a8-4a6a-a6fb-c2931d81e45c",
   "metadata": {},
   "source": [
    "### Test RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6caf0c3-55ec-4a1d-8d9b-0b4df9601f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b6afe73-1ba2-4344-a21c-8283324897bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0879841110201125"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(tt_data[:,-1], preds, squared=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
